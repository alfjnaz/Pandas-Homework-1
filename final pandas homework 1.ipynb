{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Today:\n",
    "\n",
    "0) <b>Pre-Work</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; a) Numpy Random Sampling\n",
    "\n",
    "1) <b>Pandas</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; a) Importing <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; b) Tabular Data Structures <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - from_dict() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - read_csv() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; c) <b>In-Class Exercise #1</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; d) Accessing Data <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Indexing <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - df.loc <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - keys() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Slicing a DataFrame <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; e) Built-In Methods <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - head() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - tail() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - shape <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - describe() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - sort_values() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - .columns <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; f) <b>In-Class Exercise #2</b> <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; g) Filtration <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Conditionals <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Subsetting <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; h) Column Transformations <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Generating a New Column w/Data <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - User Defined Function <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp; i) Aggregations <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - groupby() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Type of groupby() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - mean() <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - groupby() w/Multiple Columns <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - drop_duplicates() <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas <br>\n",
    "\n",
    "<p>Pandas is a flexible data analysis library built on top of NumPy that is excellent for working with tabular data. It is currently the de-facto standard for Python-based data analysis, and fluency in Pandas will do wonders for your productivity and frankly your resume. It is one of the fastest ways of getting from zero to answer in existence. </p>\n",
    "\n",
    "<ul>\n",
    "    <li>Pandas is a Python module, written in C. The Pandas module is a high performance, highly efficient, and high level data analysis library. It allows us to work with large sets of data called dataframes.</li>\n",
    "    <li>Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.)</li>\n",
    "    <li>Dataframe = Spreadsheet (has column headers, index, etc.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/alfred/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alfred/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alfred/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/alfred/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/alfred/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alfred/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# always use pd, standard for data science\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# always use pd, standard for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data structures <br>\n",
    "<p>The central object of study in Pandas is the DataFrame, which is a tabular data structure with rows and columns like an excel spreadsheet. The first point of discussion is the creation of dataframes both from native Python dictionaries, and text files through the Pandas I/O system.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from_dict()\n",
    "\n",
    "<p>Let's convert our not-so-useful-for-analysis dict into a Pandas dataframe. We can use the from_dict function to do this easily using Pandas:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/db/7c/9a60add21b96140e22465d9adf09832feade45235cd22f4cb1668a25e443/pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alfred/Library/Python/3.12/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alfred/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #1 - Read in Boston Red Sox Hitting Data <br>\n",
    "<p>Use the pandas read_csv() method to read in the statistics from the two files yesterday.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Data:\n",
      "   Rk Pos               Name  Age    G   PA   AB   R    H  2B  ...    OBP  \\\n",
      "0   1   C  Christian Vazquez   26   99  345  324  43   94  18  ...  0.330   \n",
      "1   2  1B     Mitch Moreland   31  149  576  508  73  125  34  ...  0.326   \n",
      "2   3  2B     Dustin Pedroia   33  105  463  406  46  119  19  ...  0.369   \n",
      "3   4  SS    Xander Bogaerts   24  148  635  571  94  156  32  ...  0.343   \n",
      "4   5  3B      Rafael Devers   20   58  240  222  34   63  14  ...  0.338   \n",
      "\n",
      "     SLG    OPS  OPS+   TB  GDP  HBP  SH  SF  IBB  \n",
      "0  0.404  0.735    91  131   14    3   0   1    0  \n",
      "1  0.443  0.769    99  225   14    6   0   5    6  \n",
      "2  0.392  0.760   100  159   11    2   2   4    4  \n",
      "3  0.403  0.746    95  230   17    6   0   2    6  \n",
      "4  0.482  0.819   111  107    5    0   0   0    3  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "2018 Data:\n",
      "   Rk Pos             Name  Age    G   PA   AB   R    H  2B  ...    OBP  \\\n",
      "0   1   C       Sandy Leon   29   89  288  265  30   47  12  ...  0.232   \n",
      "1   2  1B   Mitch Moreland   32  124  459  404  57   99  23  ...  0.325   \n",
      "2   3  2B    Eduardo Nunez   31  127  502  480  56  127  23  ...  0.289   \n",
      "3   4  SS  Xander Bogaerts   25  136  580  513  72  148  45  ...  0.360   \n",
      "4   5  3B    Rafael Devers   21  121  490  450  59  108  24  ...  0.298   \n",
      "\n",
      "     SLG    OPS  OPS+   TB  GDP  HBP  SH  SF  IBB  \n",
      "0  0.279  0.511    37   74    6    4   3   1    0  \n",
      "1  0.433  0.758   102  175   12    0   0   5    2  \n",
      "2  0.388  0.677    81  186   17    2   1   3    0  \n",
      "3  0.522  0.883   135  268   14    6   0   6    4  \n",
      "4  0.433  0.731    94  195    9    0   0   2    6  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_2017 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2017_hitting.txt')\n",
    "data_2018 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2018_hitting.txt')\n",
    "print(\"2017 Data:\")\n",
    "print(data_2017.head())\n",
    "print(\"\\n2018 Data:\")\n",
    "print(data_2018.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing\n",
    "\n",
    "<p>You can directly select a column of a dataframe just like you would a dict. The result is a Pandas 'Series' object.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### df.loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Along the horizontal dimension, rows of Pandas DataFrames are Row objects. You will notice there is a third column present in the DataFrame - this is the $\\textit{index}$. It is automatically generated as a row number, but can be reassigned to a column of your choice using the DataFrame.set_index(colname) method. We can use it to access particular Pandas $\\textit{rows}$, which are also Series objects:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access all of the keys/columns of the dataframe\n",
    "# Dataframe.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all data for context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In Methods <br>\n",
    "\n",
    "<p>These are methods that are frequently used when using Pandas to make your life easier. It is possible to spend a whole week simply exploring the built-in functions supported by DataFrames in Pandas. Here however, we will simply highlight a few ones that might be useful, to give you an idea of what's possible out of the box with Pandas:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataFrame.head()  -- Accepts integer parameter(gives access to more rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DataFrame.tail()  -- Accepts integer parameter(gives access to more rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe has a shape property, just like a NumPy matrix. \n",
    "# print(df.shape) -- DataFrame.shape -- No Parameter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### describe() <br>\n",
    "<p>Probably one of the most important methods to understand...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect summary statistics in one line\n",
    "# DataFrame.describe() -- Accepts parameters (include, exclude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort based on many labels, with left-to-right priority\n",
    "# sorted_data = data.sort_values('ages').reset_index()\n",
    "\n",
    "# DataFrame.sort_values('key')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### .columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will show all cols headers\n",
    "# DataFrame.columns -- has no parameters\n",
    "\n",
    "\n",
    "# Keys brings back the 'index' of whatever data type we are working with \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #2 - Describe & Sort Boston Red Sox Hitting Data <br>\n",
    "<p>Take the data that you read in earlier from the Red Sox csv's and use the describe method to understand the data better. Compare the two years and decide which team is having the better year. Then sort the values based on Batting Average.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017 had a better year based on average BA.\n",
      "\n",
      "2017 Data Sorted by BA:\n",
      "    Rk Pos                Name  Age    G   PA   AB    R    H  2B  ...    OBP  \\\n",
      "22  23  2B      Chase d'Arnaud   30    2    1    1    2    1   0  ...  1.000   \n",
      "12  13  2B       Eduardo Nunez   30   38  173  165   23   53  12  ...  0.353   \n",
      "2    3  2B      Dustin Pedroia   33  105  463  406   46  119  19  ...  0.369   \n",
      "0    1   C   Christian Vazquez   26   99  345  324   43   94  18  ...  0.330   \n",
      "4    5  3B       Rafael Devers   20   58  240  222   34   63  14  ...  0.338   \n",
      "18  19  IF     Marco Hernandez   24   21   60   58    7   16   3  ...  0.300   \n",
      "3    4  SS     Xander Bogaerts   24  148  635  571   94  156  32  ...  0.343   \n",
      "5    6  LF   Andrew Benintendi   22  151  658  573   84  155  26  ...  0.352   \n",
      "17  18  UT         Tzu-Wei Lin   23   25   66   56    7   15   0  ...  0.369   \n",
      "7    8  RF        Mookie Betts   24  153  712  628  101  166  46  ...  0.344   \n",
      "16  17  UT          Sam Travis   23   33   83   76   13   20   6  ...  0.325   \n",
      "19  20  UT         Rajai Davis   36   17   38   36    7    9   2  ...  0.289   \n",
      "1    2  1B      Mitch Moreland   31  149  576  508   73  125  34  ...  0.326   \n",
      "6    7  CF  Jackie Bradley Jr.   27  133  541  482   58  118  19  ...  0.323   \n",
      "8    9  DH      Hanley Ramirez   33  133  553  496   58  120  24  ...  0.320   \n",
      "10  11  UT         Chris Young   33   90  276  243   30   57  12  ...  0.322   \n",
      "9   10   C          Sandy Leon   28   85  301  271   32   61  14  ...  0.290   \n",
      "14  15  IF       Josh Rutledge   28   37  118  107   10   24   2  ...  0.297   \n",
      "15  16  3B      Pablo Sandoval   30   32  108   99   10   21   2  ...  0.269   \n",
      "11  12  3B       Deven Marrero   26   71  188  171   32   36   9  ...  0.259   \n",
      "21  22  UT       Blake Swihart   25    6    7    5    1    1   0  ...  0.429   \n",
      "13  14  2B          Brock Holt   29   64  164  140   20   28   6  ...  0.305   \n",
      "20  21  UT        Steve Selsky   27    8    9    9    0    1   1  ...  0.111   \n",
      "\n",
      "      SLG    OPS  OPS+   TB  GDP  HBP  SH  SF  IBB  \n",
      "22  1.000  2.000   428    1    0    0   0   0    0  \n",
      "12  0.539  0.892   128   89    3    2   0   0    0  \n",
      "2   0.392  0.760   100  159   11    2   2   4    4  \n",
      "0   0.404  0.735    91  131   14    3   0   1    0  \n",
      "4   0.482  0.819   111  107    5    0   0   0    3  \n",
      "18  0.328  0.628    65   19    0    1   0   0    0  \n",
      "3   0.403  0.746    95  230   17    6   0   2    6  \n",
      "5   0.424  0.776   102  243   16    6   1   8    7  \n",
      "17  0.339  0.709    88   19    0    0   1   0    0  \n",
      "7   0.459  0.803   108  288    9    2   0   5    9  \n",
      "16  0.342  0.667    75   26    2    1   0   0    0  \n",
      "19  0.306  0.595    56   11    2    1   0   0    0  \n",
      "1   0.443  0.769    99  225   14    6   0   5    6  \n",
      "6   0.402  0.726    89  194    8    9   0   2    4  \n",
      "8   0.429  0.750    94  213   15    6   0   0    8  \n",
      "10  0.387  0.709    85   94    4    2   0   1    0  \n",
      "9   0.354  0.644    68   96    5    1   1   3    1  \n",
      "14  0.262  0.558    49   28    1    2   0   0    0  \n",
      "15  0.354  0.622    61   35    4    0   0   1    0  \n",
      "11  0.333  0.593    54   57    8    0   3   2    0  \n",
      "21  0.200  0.629    74    1    0    0   0   0    0  \n",
      "13  0.243  0.548    47   34    3    3   0   2    0  \n",
      "20  0.222  0.333   -16    2    0    0   0   0    0  \n",
      "\n",
      "[23 rows x 28 columns]\n",
      "\n",
      "2018 Data Sorted by BA:\n",
      "    Rk Pos                Name  Age    G   PA   AB    R    H  2B  ...    OBP  \\\n",
      "7    8  RF        Mookie Betts   25  136  614  520  129  180  47  ...  0.438   \n",
      "8    9  DH       J.D. Martinez   30  150  649  569  111  188  37  ...  0.402   \n",
      "5    6  LF   Andrew Benintendi   23  148  661  579  103  168  41  ...  0.366   \n",
      "3    4  SS     Xander Bogaerts   25  136  580  513   72  148  45  ...  0.360   \n",
      "13  14  1B        Steve Pearce   35   50  165  136   19   38   8  ...  0.394   \n",
      "9   10  MI          Brock Holt   30  109  367  321   41   89  18  ...  0.362   \n",
      "2    3  2B       Eduardo Nunez   31  127  502  480   56  127  23  ...  0.289   \n",
      "12  13  UT      Hanley Ramirez   34   44  195  177   25   45   7  ...  0.313   \n",
      "15  16  SS         Tzu-Wei Lin   24   37   73   65   15   16   6  ...  0.329   \n",
      "1    2  1B      Mitch Moreland   32  124  459  404   57   99  23  ...  0.325   \n",
      "14  15  2B         Ian Kinsler   36   37  143  132   17   32   6  ...  0.294   \n",
      "4    5  3B       Rafael Devers   21  121  490  450   59  108  24  ...  0.298   \n",
      "6    7  CF  Jackie Bradley Jr.   28  144  535  474   76  111  33  ...  0.314   \n",
      "11  12  UT       Blake Swihart   26   82  207  192   28   44  10  ...  0.285   \n",
      "16  17  UT          Sam Travis   24   19   38   36    5    8   3  ...  0.263   \n",
      "10  11   C   Christian Vazquez   27   80  269  251   24   52  10  ...  0.257   \n",
      "0    1   C          Sandy Leon   29   89  288  265   30   47  12  ...  0.232   \n",
      "19  20   C          Dan Butler   31    2    7    6    0    1   0  ...  0.143   \n",
      "17  18  UT    Brandon Phillips   37    9   27   23    4    3   0  ...  0.259   \n",
      "18  19  2B      Dustin Pedroia   34    3   13   11    1    1   0  ...  0.231   \n",
      "\n",
      "      SLG    OPS  OPS+   TB  GDP  HBP  SH  SF  IBB  \n",
      "7   0.640  1.078   186  333    5    8   0   5    8  \n",
      "8   0.629  1.031   173  358   19    4   0   7   11  \n",
      "5   0.465  0.830   123  269    9    2   2   7    1  \n",
      "3   0.522  0.883   135  268   14    6   0   6    4  \n",
      "13  0.507  0.901   141   69    1    5   0   2    0  \n",
      "9   0.411  0.774   109  132    7    7   0   2    2  \n",
      "2   0.388  0.677    81  186   17    2   1   3    0  \n",
      "12  0.395  0.708    90   70    9    2   0   2    0  \n",
      "15  0.415  0.744   100   27    0    0   0   0    0  \n",
      "1   0.433  0.758   102  175   12    0   0   5    2  \n",
      "14  0.311  0.604    64   41    5    0   0   1    0  \n",
      "4   0.433  0.731    94  195    9    0   0   2    6  \n",
      "6   0.403  0.717    92  191    6   11   0   4    3  \n",
      "11  0.328  0.613    65   63    4    0   0   0    0  \n",
      "16  0.389  0.652    73   14    1    0   0   0    0  \n",
      "10  0.283  0.540    46   71    5    4   1   0    1  \n",
      "0   0.279  0.511    37   74    6    4   3   1    0  \n",
      "19  0.167  0.310   -17    1    0    0   0   1    0  \n",
      "17  0.261  0.520    42    6    1    0   0   0    0  \n",
      "18  0.091  0.322    -7    1    0    0   0   0    0  \n",
      "\n",
      "[20 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_2017 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2017_hitting.txt')\n",
    "data_2018 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2018_hitting.txt')\n",
    "desc_2017 = data_2017.describe()\n",
    "desc_2018 = data_2018.describe()\n",
    "if desc_2017.loc['mean', 'BA'] > desc_2018.loc['mean', 'BA']:\n",
    "    print(\"\\n2017 had a better year based on average BA.\")\n",
    "elif desc_2017.loc['mean', 'BA'] < desc_2018.loc['mean', 'BA']:\n",
    "    print(\"\\n2018 had a better year based on average BA.\")\n",
    "else:\n",
    "    print(\"\\nBoth years had the same average BA.\")\n",
    "sorted_2017 = data_2017.sort_values(by='BA', ascending=False)\n",
    "sorted_2018 = data_2018.sort_values(by='BA', ascending=False)\n",
    "print(\"\\n2017 Data Sorted by BA:\")\n",
    "print(sorted_2017)\n",
    "print(\"\\n2018 Data Sorted by BA:\")\n",
    "print(sorted_2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtration <br>\n",
    "<p>Let's look at how to filter dataframes for rows that fulfill a specific conditon.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional boolean dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exactly like numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformations <br>\n",
    "<p>Rarely, if ever, will the columns in the original raw dataframe read from CSV or database table be the ones you actually need for your analysis. You will spend lots of time constantly transforming columns or groups of columns using general computational operations to produce new ones that are functions of the old ones. Pandas has full support for this: Consider the following dataframe containing membership term and renewal number for a group of customers:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating a New Column w/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame['key'] = Some Calculation from our DataFrame Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User Defined Function\n",
    "\n",
    "<p>If what you want to do to a column that can't be represented by simple mathematical operations, you can write your own $\\textit{user defined function}$ with the full customizability available in Python and any external Python packages, then map it directly onto a column. Let's add some ages to our customer dataframe, and then classify them into our custom defined grouping scheme:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use .apply to map over dataframe\n",
    "\n",
    "# Create a new column for ages\n",
    "# customers['ages'] = np.random.randint(18,70,10)\n",
    "\n",
    "# User defined function\n",
    "\n",
    "    \n",
    "# print(make_age_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As a last example I'll show here how you would use a lambda function to create a UDF that depends on $\\textit{more than one}$ column:</p>\n",
    "\n",
    "<li>UDF = User Defined Function</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Axis for apply can only be 1 or 0 -- 1 being the X axis 0 being the Y axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #3 - Create Your Own UDF <br>\n",
    "<p>Using the Boston Red Sox data, create your own UDF which creates a new column called 'All-Star' and puts every player with either a batting average over .280 or an on base percentage of over .360 with a result of 'Yes' in the column and 'No' if not.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Data:\n",
      "                  Name     BA    OBP All-Star\n",
      "0    Christian Vazquez  0.290  0.330      Yes\n",
      "1       Mitch Moreland  0.246  0.326       No\n",
      "2       Dustin Pedroia  0.293  0.369      Yes\n",
      "3      Xander Bogaerts  0.273  0.343       No\n",
      "4        Rafael Devers  0.284  0.338      Yes\n",
      "5    Andrew Benintendi  0.271  0.352       No\n",
      "6   Jackie Bradley Jr.  0.245  0.323       No\n",
      "7         Mookie Betts  0.264  0.344       No\n",
      "8       Hanley Ramirez  0.242  0.320       No\n",
      "9           Sandy Leon  0.225  0.290       No\n",
      "10         Chris Young  0.235  0.322       No\n",
      "11       Deven Marrero  0.211  0.259       No\n",
      "12       Eduardo Nunez  0.321  0.353      Yes\n",
      "13          Brock Holt  0.200  0.305       No\n",
      "14       Josh Rutledge  0.224  0.297       No\n",
      "15      Pablo Sandoval  0.212  0.269       No\n",
      "16          Sam Travis  0.263  0.325       No\n",
      "17         Tzu-Wei Lin  0.268  0.369      Yes\n",
      "18     Marco Hernandez  0.276  0.300       No\n",
      "19         Rajai Davis  0.250  0.289       No\n",
      "20        Steve Selsky  0.111  0.111       No\n",
      "21       Blake Swihart  0.200  0.429      Yes\n",
      "22      Chase d'Arnaud  1.000  1.000      Yes\n",
      "\n",
      "2018 Data:\n",
      "                  Name     BA    OBP All-Star\n",
      "0           Sandy Leon  0.177  0.232       No\n",
      "1       Mitch Moreland  0.245  0.325       No\n",
      "2        Eduardo Nunez  0.265  0.289       No\n",
      "3      Xander Bogaerts  0.288  0.360      Yes\n",
      "4        Rafael Devers  0.240  0.298       No\n",
      "5    Andrew Benintendi  0.290  0.366      Yes\n",
      "6   Jackie Bradley Jr.  0.234  0.314       No\n",
      "7         Mookie Betts  0.346  0.438      Yes\n",
      "8        J.D. Martinez  0.330  0.402      Yes\n",
      "9           Brock Holt  0.277  0.362      Yes\n",
      "10   Christian Vazquez  0.207  0.257       No\n",
      "11       Blake Swihart  0.229  0.285       No\n",
      "12      Hanley Ramirez  0.254  0.313       No\n",
      "13        Steve Pearce  0.279  0.394      Yes\n",
      "14         Ian Kinsler  0.242  0.294       No\n",
      "15         Tzu-Wei Lin  0.246  0.329       No\n",
      "16          Sam Travis  0.222  0.263       No\n",
      "17    Brandon Phillips  0.130  0.259       No\n",
      "18      Dustin Pedroia  0.091  0.231       No\n",
      "19          Dan Butler  0.167  0.143       No\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Name  BA OBP AllStar\n",
    "    --------------------\n",
    "    Name .233 .360 Yes\n",
    "    Name .150 .288 No\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "data_2017 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2017_hitting.txt')\n",
    "data_2018 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2018_hitting.txt')\n",
    "def is_all_star(row):\n",
    "    if row['BA'] > 0.280 or row['OBP'] > 0.360:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "data_2017['All-Star'] = data_2017.apply(is_all_star, axis=1)\n",
    "data_2018['All-Star'] = data_2018.apply(is_all_star, axis=1)\n",
    "selected_columns = ['Name', 'BA', 'OBP', 'All-Star']\n",
    "data_2017 = data_2017[selected_columns]\n",
    "data_2018 = data_2018[selected_columns]\n",
    "print(\"2017 Data:\")\n",
    "print(data_2017)\n",
    "print(\"\\n2018 Data:\")\n",
    "print(data_2018)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations <br>\n",
    "<p>The raw data plus some transformations is generally only half the story. Your objective is to extract actual insights and actionable conclusions from the data, and that means reducing it from potentially billions of rows to some summary statistics via aggregation functions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### groupby() <br>\n",
    "<p>The .groupby() function is in some ways a 'master' aggregation.</p> \n",
    "\n",
    "<p>Data tables will usually reserve one column as a primary key - that is, a column for which each row has a unique value. This is to facilitate access to the exact rows of a data table that a user wants to view. The other columns will often have repeated values, such as the age groups in the above examples. We can use these columns to explore the data using the Pandas API:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also introducing .count() here, exact same as to how it's used in SQL\n",
    "\n",
    "# Using the groupby with the column intact as a column/key\n",
    "# customers.groupby('age_group', as_index = False).count()[['customer_id','age_group']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type of groupby()\n",
    "\n",
    "<p>The result is a new dataframe, the columns of which all contain the counts of the grouped field. Notice the type of a grouped dataframe:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is because simply grouping data doesn't quite make sense without an aggregation function like count() to pair with. In this case, we're counting occurances of the grouped field, but that's not all we can do. We can take averages, standard deviations, mins, maxes and much more! Let's see how this works a bit more:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### groupby() w/Multiple Columns\n",
    "\n",
    "<p>We end up with the average age of the groups in the last column, the average tenure in the tenure column, and so on and so forth. You can even split the groups more finely by passing a list of columns to group by:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop_duplicates()\n",
    "\n",
    "<p>Drops all duplicates from the current dataframe</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send customer data into a CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Thus the groupby operation allows you to rapidly make summary observations about the state of your entire dataset at flexible granularity. In one line above, we actually did something very complicated - that's the power of the dataframe. In fact, the process often consists of several iterative groupby operations, each revealing greater insight than the last - if you don't know where to start with a dataset, try a bunch of groupbys!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Excersise #1 - Find the Total Number of Runs and RBIs for the Red Sox <br>\n",
    "<p>Get total number of home runs and rbi's</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Data:\n",
      "       HR  RBI\n",
      "Team          \n",
      "BOS   168  735\n",
      "\n",
      "2018 Data:\n",
      "       HR  RBI\n",
      "Team          \n",
      "BOS   208  826\n"
     ]
    }
   ],
   "source": [
    "# step 1: Add a new column with the key 'Team' and all column values should be 'BOS'\n",
    "\n",
    "# step 2: Group by the 'Team' column and get total home runs and rbi's\n",
    "\n",
    "# Produce data for both 2017 and 2018 (ie print both seperated by a newline character \\n)\n",
    "\n",
    "\"\"\"\n",
    "TEAM    HR   RBI\n",
    "----------------\n",
    "BOS     144  538\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "data_2017 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2017_hitting.txt')\n",
    "data_2018 = pd.read_csv('/Users/alfred/Downloads/Numpy Data Analysis - Day 1/redsox_2018_hitting.txt')\n",
    "data_2017['Team'] = 'BOS'\n",
    "data_2018['Team'] = 'BOS'\n",
    "grouped_2017 = data_2017.groupby('Team').sum()[['HR', 'RBI']]\n",
    "grouped_2018 = data_2018.groupby('Team').sum()[['HR', 'RBI']]\n",
    "print(\"2017 Data:\")\n",
    "print(grouped_2017)\n",
    "print(\"\\n2018 Data:\")\n",
    "print(grouped_2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the average age of runners in the 2017 Boston Marathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average age of runners in the 2017 Boston Marathon was 42.59\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/alfred/Downloads/marathon_results_2017.csv')\n",
    "average_age = data['Age'].mean()\n",
    "print(f\"The average age of runners in the 2017 Boston Marathon was {average_age:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
